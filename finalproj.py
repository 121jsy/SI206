'''
Project: SI 206 Winter 2025 Project, APIs, SQL, and Visualizations
Team Members:
    - Joonseo Yoon / joonseoy
    - Shea Shin / yoonseos
References (Generative AI, Websites, etc.):
    - Kaggle API (kagglehub): https://github.com/Kaggle/kagglehub?tab=readme-ov-file#user-content-fn-2-e8c07f4a8c03b71229d22ad6a451a240
    - Reddit API: https://www.reddit.com/dev/api/
    - Reddit API PRAW: https://praw.readthedocs.io/en/stable/

Project Description:
    Uses Kaggle API to retrieve "Top Spotify Songs in 73 Countries" dataset and Reddit API to observe
    the popularity of the songs reflected on one of the most popular social media, Reddit.

Questions:
    - What is the best way to calculate the sentiment "score" of the articles? 
      (Just the headlines or the whole article? Or a combination of both? How do we calculate the score?)
    - Is the sentiment score accurate?
    - How do we visualize the results in a meaningful way?

APIs Used:
    - Kaggle API (kagglehub)
    - Reddit API (PRAW?)

Visualization Tools:
    - Plotly
    - Pandas
'''
import praw

import os
import sqlite3
import json
import pandas as pd
import requests
from datetime import datetime
import time

import config

import kagglehub
from kagglehub import KaggleDatasetAdapter

# Using OAuth to increase rate limit
reddit = praw.Reddit(
    client_id=config.REDDIT_CLIENT_ID,
    client_secret=config.REDDIT_CLIENT_SECRET,
    user_agent=config.REDDIT_USER_AGENT
)

def load_kaggle_dataset(criteria, option="1"):
    '''
    Loads kaggle dataset (Top Spotify Songs in 73 Countries (Daily Updated)) using Kaggle public API.
    Saves the dataset in the current directory as a .json file, or stores it as a python object 
    depending on user choice.

    ARGUMENTS:
        option (str): An optional argument that indicates the loading/saving option.
        criteria (dict): Argument that decides what data to keep (e.g. {"country": "US", "data_start": "2025-04-01"})

    RETURNS:
        json_object: A dataset in json format returned with option=1. 
        None: Returned with option=2. json file saved in current directory
    '''

    current_directory = os.path.dirname(os.path.abspath(__file__))

    # Load Kaggle dataset into dataframe python object
    df = kagglehub.dataset_load(
        KaggleDatasetAdapter.PANDAS,
        "asaniczka/top-spotify-songs-in-73-countries-daily-updated",
        "universal_top_spotify_songs.csv",
        pandas_kwargs={"usecols": ["name", "artists", "daily_rank", "country", "snapshot_date", "popularity"]}
    )

    if df is not None:

        print("Dataset loaded successfully.\n")

        # Filter the dataframe based on criteria
        filtered_df = df
        for criteria_key, criteria_val in criteria.items():
            filtered_df = filtered_df.loc[(filtered_df[criteria_key] == criteria_val)]

        # TESTING - display whole dataset
        print(filtered_df)
        print()

        # Option 1: Convert the dataset to .json (for project's purpose) format and keep as a python object
        if option == "1":
            print("Option 1 (default): Saving dataset as json format python object")
            json_string = filtered_df.to_json(orient='records', lines=False)
            json_object = json.loads(json_string)

            create_update_kaggle_db(json_object)

            return json_object

        # Option 2: Convert the dataset to .json format (for project's purpose) and save it in the current directory
        elif option == "2":
            filtered_df.to_json(os.path.join(current_directory, 'universal_top_spotify_songs.json'), 
                    orient='records', 
                    lines=False)
            
            update_database(filename='universal_top_spotify_songs.json')
    else:
        print("Failed to load dataset.\n")



'''
ê¸°ì¡´ì— ì•ˆì“°ì´ë˜ function ì—…ë°ì´íŠ¸: Music table (id, unique music name), KaggleData table (ì¡ë‹¤í•œ ì •ë³´+ Music tableì˜ 
id foreign keyë¡œ reference) ìƒì„±. ë‚˜ì¤‘ì— ê³ ë ¤í•´ì•¼ ë  ê²ƒë“¤: 
1. KaggleData tableì˜ id, music_id, daily_rankê°€ ë‹¤ ë˜‘ê°™ìŒ
2. country, snapshot_date columnì— ìˆëŠ” ê°’ë“¤ ë‹¤ ë˜‘ê°™ìŒ
3. snapshot_dateëŠ” ì•„ì˜ˆ ì—†ì• ê³ , countryëŠ” ìƒˆë¡œìš´ tableì„ ë§Œë“¤ê³  foreign keyë¡œ referenceí•˜ëŠ”ê²Œ ì–´ë–¨ì§€?
'''
def create_update_kaggle_db(json_object=None):
    '''
    Updates the database SQLite database with the json data or filename. If the data is
    provided, it will insert the data directly into the database. If the filename is provided,
    it will read the file and insert the data into the database.

    ARGUMENTS:
        json_object (dict): A dictionary containing the data retrieved with Kaggle API
    '''

    path = os.path.dirname(os.path.abspath(__file__))
    conn = sqlite3.connect(path + "/reddit.db")
    cur = conn.cursor()

    # Create Music table (unique music name and id)
    cur.execute('''
        CREATE TABLE IF NOT EXISTS Music (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT UNIQUE
        )
    ''')
    # Create KaggleData table (linked to Music table with FOREIGN KEY)
    cur.execute('''
        CREATE TABLE IF NOT EXISTS KaggleData (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            music_id INTEGER UNIQUE,
            daily_rank INTEGER,
            country TEXT,
            snapshot_date TEXT,
            popularity INTEGER,
            FOREIGN KEY (music_id) REFERENCES Music(id)
        )
    ''')

    # Insert the data into the Music table and KaggleData table
    if json_object is not None:
        count = 0
        for music in json_object:
            if count >= 25:
                conn.commit()
                return
            name = music["name"]

            # To avoid inconsistent id, check if song exists in the Music table
            # Without this step, the "id" will be skipped (e.g. 24, 25, 51, 52, ...)
            cur.execute("SELECT id FROM Music WHERE name = ?", (name,))
            row = cur.fetchone()
            if row is None:
                cur.execute("INSERT OR IGNORE INTO Music (name) VALUES (?)", (name,))
                music_id = cur.lastrowid
            else:
                music_id = row[0]

            # Get the corresponding music_id from the Music table
            cur.execute("SELECT id FROM Music WHERE name = ?", (name,))
            music_id = cur.fetchone()[0]
            # Insert the rest of the data into the KaggleData table
            daily_rank = music["daily_rank"]
            country = music["country"]
            snapshot_date = music["snapshot_date"]
            popularity = music["popularity"]

            cur.execute("INSERT OR IGNORE INTO KaggleData (music_id, daily_rank, country, snapshot_date, popularity) VALUES (?, ?, ?, ?, ?)",
                        (music_id, daily_rank, country, snapshot_date, popularity))
            
            if cur.rowcount == 1:   # rowcount property returns the affected by the previous execute()
                count += 1          # Thus, if the affected (newly inserted) row is 1, increment count

    conn.commit()
    conn.close()


# def search_reddit_data():
#     # TODO: Decide whether to use Reddit API or PRAW, and implement the search function
#     # Also consider how to store the data into database: each music gets mention frequency 
#     # data or store them intoseparate table
#     credentials = get_reddit_api_auth()
#     id = credentials['client_id']
#     secret = credentials['client_secret']
#     reddit = praw.Reddit(
#     client_id=id,
#     client_secret=secret,
#     user_agent='si206:v1.2.3 (by u/Grand_Compote2026)' )

#     # print(reddit.read_only)
#     # for submission in reddit.subreddit("music").hot(limit=10):
#     #     print(submission.title)

#     # subreddit = reddit.subreddit("music")
#     # keyword = "Sabrina Carpenter"
#     # count = 0
#     # for submission in subreddit.new(limit=100):  # or .hot/.top/.search etc.
#     #     if keyword.lower() in submission.title.lower() or keyword.lower() in submission.selftext.lower():
#     #         count += 1
#     # print(f"'{keyword}' mentioned in {count} recent posts on r/music")

#     # return count
#     subreddit = "music"
#     keyword = "Taylor Swift"
#     start_date = "2024-12-01"
#     end_date = "2025-04-01"
#     subreddit = reddit.subreddit("music")
#     start_ts = int(datetime.strptime(start_date, "%Y-%m-%d").timestamp())
#     end_ts = int(datetime.strptime(end_date, "%Y-%m-%d").timestamp())

#     count = 0
#     # for submission in subreddit.hot(time_filter="all"):
#     #     post_ts = int(submission.created_utc)
#     #     # if start_ts <= post_ts <= end_ts:
#     #     title = submission.title.lower()
#     #     body = submission.selftext.lower()
#     #     if keyword.lower() in title or keyword.lower() in body:
#     #         count += 1
#     #         print(submission.title)
#     # print(f"Found {count} posts mentioning '{keyword}' in r/{subreddit} between {start_date} and {end_date}.")

#     count = 0
#     keyword = "sabrina carpenter"
#     for submission in reddit.subreddit("Music+hiphopheads+popheads").top(time_filter="month"):
#         if keyword.lower() in submission.title.lower() or keyword.lower() in submission.selftext.lower():
#            count += 1
#            print(submission.title)
#            print(f"count : {count}")

#     return count

"""
ë‚˜ì¤‘ì— DB ë‹¤ ì™„ì„±ë˜ë©´ ì“¸ function
Count reddit posts containing specific keyword in title or text by selecting data from Reddit DB
"""
def count_reddit_posts(keyword):
   pass
    
"""
ìƒˆë¡œ ë§Œë“¦
Set up database in local directory and returns cursor and connection objects
"""
def setup_db(db_name):
    path = os.path.dirname(os.path.abspath(__file__))
    conn = sqlite3.connect(path + "/" + db_name)
    cur = conn.cursor()
    return cur, conn
            
"""
Create Reddit database using passed cursor and connection objects
It stores less than 25 items each time it's called and makes sure there's no duplicates -> ì´ê±´ ì•„ì§ ëª¨ë¥´ê² ìŒ
25 ê°œì”© ë„£ëŠ” logic ì•½ê°„ ìˆ˜ì •, Music tableì— ìˆëŠ” idë¥¼ reference í•´ì„œ ì‚½ì…
"""
def create_update_reddit_db(cur, conn, post_dict):
    cur.execute('''
        CREATE TABLE IF NOT EXISTS Reddit (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT UNIQUE,
            music_id INTEGER,
            FOREIGN KEY (music_id) REFERENCES Music(id)
        )
    ''')

    count = 0

    for song, posts in post_dict.items():
        for post in posts:
            if count >= 25:
                conn.commit()
                return
            cur.execute("SELECT id FROM Music WHERE name = ?", (song,))
            music_id = cur.fetchone()[0]
            cur.execute("INSERT OR IGNORE INTO Reddit (title, music_id) VALUES (?, ?)",
                        (post['title'], music_id))
            if cur.rowcount == 1:   # rowcount property returns the affected by the previous execute()
                count += 1          # Thus, if the affected (newly inserted) row is 1, increment count

    conn.commit()

'''
Reddit OAuthë‘ PRAWì‚¬ìš©í•´ì„œ rate limit ëŠ˜ë¦¬ê³  data retrieval efficiency ìµœëŒ€í•œ ëŠ˜ë¦° ë²„ì „
1. search_reddit_posts() ì—ì„œëŠ” json_dataë¥¼ ë°›ê³ , ê·¸ê±¸ ì½ì–´ì„œ ë…¸ë˜ë¥¼ 5ê°œì”© ë¬¶ì€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“  í›„, group_search()ì— ë„˜ê²¨ì¤Œ
2. group_search()ì—ì„œëŠ” ë…¸ë˜ ì œëª©ë“¤ì„ ë°›ê³ , subredditì„ ê·¸ë£¨í•‘ í•œ í›„ ê·¸ ê·¸ë£¹ì˜ subredditì—ì„œ ê²€ìƒ‰, ê·¸ ê²°ê³¼ë¥¼ ë‹¤ì‹œ search_reddit_posts()ì— ë„˜ê²¨ì¤Œ
3. group_search()ì—ì„œ ë¦¬í„´ëœ ê²°ê³¼ë¥¼ search_reddit_posts()ì´ ë°›ìœ¼ë©´ song_postsì— ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ì‹
'''
def search_reddit_posts_v1(json_data):
    """
    Retrieves Reddit posts mentioning each song in the provided JSON data.
    Groups 5 songs together per Reddit API search to be more efficient.
    """

    # {"song_name1": [{post1 data}, {post2 data}, ...], "song_name2": [{post1}, {post2}, ...], ...}
    song_posts = {}

    grouping_size = 5   # Number of songs to group for efficient search
    total_songs = len(json_data)

    # Increase the start_index by +grouping_size (5) each time, until it reaches total_songs (100)
    # Updates the grouped_songs list with 5 songs each time
    for start_index in range(0, total_songs, grouping_size):
        grouped_songs = [] # ["song_name1", "song_name2", ..., "song_name5"]
        for i in range(start_index, start_index + grouping_size):
            grouped_songs.append(json_data[i]["name"])

        group_search_results = group_search(grouped_songs)

        for song_name, posts in group_search_results.items():
            song_posts[song_name] = posts

        time.sleep(0.6)

    return song_posts

def group_search(song_names, max_posts=100):
    """
    Searches for Reddit posts containing the group of song names in the chosen subreddit group. 
    Seaches from the top posts of past month.

    ARGUMENTS:
        song_names (list): A list of song names to search for.
        max_posts (int): The maximum number of posts to retrieve.
    RETURNS:
        posts_by_song (dict): A dictionary where keys are song names and values are lists of Reddit posts
        containing those song names.
    """
    # Group up the subreddits to search in
    subreddit_group = "Music+hiphopheads+popheads+popculturechat"
    query = " OR ".join([f'"{name}"' for name in song_names])

    # Prepopulated dictionary 
    # {"song_name1": [{post1 data}, {post2 data}, ...], "song_name2": [{post1}, {post2}, ...], ...}
    posts_by_song = {}
    for name in song_names:
        posts_by_song[name] = []

    subreddit = reddit.subreddit(subreddit_group)
    # Search for posts in the chosen subreddits
    for post in subreddit.search(query, sort="top", time_filter="month", limit=max_posts):
        # Text of the title and selftext of the post (lowercased for proper match count)
        text = (post.title + " " + post.selftext).lower()

        # Check if the post contains the music name. If so, insert into dictionary
        for song in song_names:
            if song.lower() in text:
                post_data = {
                    "id": post.id,
                    "title": post.title,
                }
                posts_by_song[song].append(post_data)

    return posts_by_song


"""
ìƒˆë¡œ ìˆ˜ì •í•œ function
Search reddit posts containing specific keyword
It does not count the number of posts, but just returns the dictionary with fetched data. -> update the dictionary
keyëŠ” keyword(i.e. music name)ì´ê³  keyword ë‹¹ ëª¨ë“  subredditì—ì„œ matchingëœ post dataë¥¼ ì €ì¥í•œ listë¥¼ valueë¡œ í•¨
"""
def search_reddit_posts(keyword, post_dict, max_posts_per_sub=400):
    headers = {
        "User-Agent": "KeywordTracker/1.0 by si206final"
    }
    data_list = []
    subreddits = ["popheads", "Music", "hiphopheads", "popculturechat"] #popular music relevant subreddits

    for sub in subreddits:
        print(f"\nğŸ” Searching r/{sub} for keyword: '{keyword}'...")
        after = None
        fetched = 0
        count = 0

        while fetched < max_posts_per_sub:
            url = f"https://www.reddit.com/r/{sub}/search.json"
            params = {
                "q": keyword,
                "restrict_sr": "on", #only search in the subreddit
                "sort": "top",
                "t": "month",
                "limit": 100, #number of items to fetch
            }
            if after:
                params["after"] = after

            response = requests.get(url, headers=headers, params=params)
            if response.status_code != 200:
                print(f"Error fetching from r/{sub}: {response.status_code}")
                break

            data = response.json().get("data", {})
            children = data.get("children", [])

            if not children: #no posts
                break

            for child in children:
                post = child["data"]
                # Check keyword in title or text
                if keyword.lower() in (post.get("title", "").lower() + " " + post.get("selftext", "").lower()):
                    data_list.append(post)
                    count += 1

            fetched += len(children)
            after = data.get("after") #to get more than 100 results
            #If after is None or missing, that means: You've reached the last page of results.

            if not after:
                break

            time.sleep(2)  # avoid rate-limiting 
        print(f"âœ… Found {count} matching posts in r/{sub}")    
    # print(f"data list size is {len(data_list)}")
    post_dict[keyword] = data_list # ex) {"NOKIA" : [{post1}, {post2}, {post3} ... ] }
    return post_dict

"""
Search reddit posts in past month that contain specific keyword in title or text.
It returns the total count of posts
"""
# def search_reddit_posts(keyword, max_posts_per_sub=400):
#     headers = {
#         "User-Agent": "KeywordTracker/1.0 by si206final"
#     }

#     all_results = {}
#     total_count = 0

#     '''
#     for (all kaggle json data):
#         if (the song critera chosen == true (e.g. [song1, song2, ..., song25]))
            
#     '''
#     subreddits = ["popheads", "Music", "hiphopheads", "popculturechat"] #popular music relevant subreddits

#     for sub in subreddits:
#         print(f"\nğŸ” Searching r/{sub} for keyword: '{keyword}'...")
#         posts = []
#         after = None
#         fetched = 0

#         while fetched < max_posts_per_sub:
#             url = f"https://www.reddit.com/r/{sub}/search.json"
#             params = {
#                 "q": keyword,
#                 "restrict_sr": "on", #only search in the subreddit
#                 "sort": "top",
#                 "t": "month",
#                 "limit": 100, #number of items to fetch
#             }
#             if after:
#                 params["after"] = after

#             response = requests.get(url, headers=headers, params=params)
#             if response.status_code != 200:
#                 print(f"Error fetching from r/{sub}: {response.status_code}")
#                 break

#             data = response.json().get("data", {})
#             children = data.get("children", [])

#             if not children:
#                 break

#             for child in children:
#                 post = child["data"]
#                 # Check keyword in title or text
#                 if keyword.lower() in (post.get("title", "").lower() + " " + post.get("selftext", "").lower()):
#                     posts.append(post)

#             fetched += len(children)
#             after = data.get("after") #to get more than 100 results
#             #If after is None or missing, that means: You've reached the last page of results.

#             if not after:
#                 break

#             time.sleep(2)  # avoid rate-limiting

#         print(f"âœ… Found {len(posts)} matching posts in r/{sub}")
#         total_count += len(posts)
#         all_results[sub] = posts
    
#     return total_count

# def filter_by_date(posts, start_date, end_date):
#     """
#     Filters posts to only include those within the date range.
#     start_date and end_date should be in 'YYYY-MM-DD' format.
#     """
#     start_ts = int(time.mktime(datetime.strptime(start_date, "%Y-%m-%d").timetuple()))
#     end_ts = int(time.mktime(datetime.strptime(end_date, "%Y-%m-%d").timetuple()))

#     filtered = []
#     for post in posts:
#         post_time = post.get("created_utc")
#         if post_time and start_ts <= post_time <= end_ts:
#             filtered.append(post)
#     return filtered



def main():
    # ========================================================================================
    print("===================================================================================")
    print("SI 206 W25 Final Project")
    print("Music trend analysis with Kaggle and Reddit API")
    print("===================================================================================\n")
    # print("--------------------------------------------------------")
    # print("-----------------------------------------------------------------------------------\n")

    load_options = '''Dataset loading options:
    1: python object
    2: local json file\n'''
    print(load_options)

    load_option = input("Please select an option (1 or 2): ")
    print()

    options = '''Options:
    1. Option 1: Spotify Daily Rank vs. Reddit Mention Frequency
    2. Option 2: Spotify Popularity vs. Reddit Mention Frequency
    3. Option 3: Spotify Music's Artist vs. Reddit Mention Frequency
    4. Option 4: Explit Music Reddit Mention. Non-Explicit Music Reddit Mention
    5. Option 5: ??? Reddit Sentiment ??? (EC using additional API, e.g. TextBlob)
    6. Option 6: ???
    7. Exit\n'''
    
    print(options)

    option = 0

    while option != "7":
        # get user input
        option = input("Please select an option (1-7): ")
        print()

        if option == "1":
            print("Option 1: Top Charts \n")
            # keyword = input("Keyword/phrase to search for: ")
            criteria = {
                "country": "US",
                "snapshot_date": "2025-04-18"
            }
            json_object = load_kaggle_dataset(criteria, load_option)

            # ê¸°ì¡´ ë°©ì‹
            '''
            post_dict = {} # music_nameì„ keyë¡œ í•˜ê³  ê·¸ì— í•´ë‹¹í•˜ëŠ” [{post1}, {post2}, {post3} ...]ì„ valueë¡œ í•˜ëŠ” dict, postëŠ” ëª¨ë“  subredditì—ì„œ ì„œì¹˜ëœ í¬ìŠ¤ë“œë“¤ì„
            keyword = json_object[0]["name"] #ì¼ë‹¨ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì²«ë²ˆì§¸ ë…¸ë˜ë§Œ í•´ë´„
            post_dict = search_reddit_posts(keyword, post_dict) #fetched post data
            cur, conn = setup_db("reddit.db") #Reddit DB ë§Œë“¤ê¸°
            create_update_reddit_db(cur, conn, post_dict) #Reddit DB update 
            '''

            # 
            cur, conn = setup_db("reddit.db") #Reddit DB ë§Œë“¤ê¸°
            song_post_dict = search_reddit_posts_v1(json_object) #fetched post data
            create_update_reddit_db(cur, conn, song_post_dict) #Reddit DB update 

            """
            ì°¸ê³ ë¡œ ë°‘ì— url ë“¤ì–´ê°€ë©´ api call í–ˆì„ ë•Œ ì–´ë–¤ ì‹ìœ¼ë¡œ jsonì´ ë¦¬í„´ë˜ëŠ”ì§€ ë°”ë¡œ ë³¼ ìˆ˜ ìˆì–´.
            https://www.reddit.com/r/Music/search.json?q=NOKIA&restrict_sr=on&sort=top&t=month&limit=100

            -ì•ìœ¼ë¡œ ë” í•´ì•¼í•  ê²ƒ-
            1. Reddit tableì— id INTEGER PRIMARY KEY AUTOINCREMENT ì„¤ì •í•˜ê¸° -> í•´ê²°
            2. music_id DB ë§Œë“¤ê¸° -> ì§€ê¸ˆ Reddit DBì— ìˆëŠ” duplicate stringë“¤ì„ int id ë¡œ ë°”ê¾¸ê¸° -> í•´ê²°
            3. ëª¨ë“  ë…¸ë˜ì— ëŒ€í•´ì„œ Reddit db ì—…ë°ì´íŠ¸ í•˜ê¸°  -> í•´ê²°
            4. kaggle DB ë§Œë“¤ê¸° -> í•´ê²°

            -directory ë³€ê²½ì‚¬í•­- 
            1. config.pyì— REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT ë„£ê¸° <- ì´ê±´ ë‚´ì¼ ì•Œë ¤ì¤„ê²Œ

            -í• ê±° ì¶”ê°€!- 
            1. Duplicate String ì¶”ê°€ í™•ì¸í•˜ê¸° (e.g. KaggleDataì— music_id, daily_rank, country, snapshot_date ë‹¤ ë˜‘ê°™ìŒ)
            2. ì¼ìë³„ + ê° êµ­ê°€ë³„ ë…¸ë˜ê°€ 50ê°œì”©ì´ë¼ "100 rows per API"ì•„ì§ ì¶©ì¡± ëª»í•¨ -> ë‹¤ë¥¸ ë‚˜ë¼ ë…¸ë˜ ê°€ì ¸ì˜¤ëŠ” ê²ƒ ê³ ë ¤
                - ë‘ êµ­ê°€ì˜ top 50ì—ì„œ ì¤‘ë³µë˜ëŠ” ë…¸ë˜ê°€ ìˆì„ ìˆ˜ ìˆëŠ”ë°, ë…¸ë˜ ì œëª©ì€ music_idë¡œ ë°”ê¿”ì„œ ì €ì¥í•˜ë‹ˆê¹Œ redundant dataëŠ” ë”±íˆ
                  ê±±ì • ì•ˆí•´ë„ ë  ë“¯. ì¶”ê°€ë¡œ Country tableê¹Œì§€ ë§Œë“¤ì–´ì„œ foreign keyë¡œ referenceí•˜ë©´ ê°™ì€ ê³¡ì´ì–´ë„ êµ¬ë¶„ ê°€ëŠ¥
            3. KaggleDataì— 100 rows ë‹¤ ì €ì¥í•˜ë©´ mention count êµ¬í•˜ê¸°
            4. setup_db() ë‚´ì—ì„œ create_update_kaggle_db() ë° create_update_reddit_db() í˜¸ì¶œí•˜ê¸°
                - ë” ê¹”ë”í•¨. ì´ë ‡ê²Œ í•˜ë ¤ë©´ load_kaggle_dataset()ì—ì„œ create_update_kaggle_db() í˜¸ì¶œí•˜ì§€ ë§ê²ƒ
                - create_update_kaggle_db()ì—ì„œ return ë°›ì€ ê°’ì„ curr, connì´ë‘ ê°™ì´ ë„˜ê²¨ì£¼ëŠ” ë°©ì‹?
            5. ì½”ë“œ ì •ë¦¬
            6. Calculation functionë“¤ ì‘ì„±
            7. Visualization functionë“¤ ì‘ì„±
            8. ë¦¬í¬íŠ¸ ì‘ì„±
            """

            # total = 0
            # print("\nğŸ“Š Summary:")
            # for name, posts in results.items():
            #     print(f"{name} with {posts} posts")
                

        elif option == "2":
            print("Option 2: \n")
        elif option == "3":
            print("Option 3: \n")
        elif option == "4":
            print("Option 4: \n")
        elif option == "5":
            print("Option 5: \n")
        elif option == "6":
            print("Option 6: All above\n")
        elif option == "7":
            print("Exiting the program...\n")
            return
        else:
            print("INVALID OPTION\n")
            print(options)

        print("-----------------------------------------------------------------------------------\n")


# def main():
    # subreddits = ["popheads", "Music", "hiphopheads", "popculturechat"] #popular music relevant subreddits
    # keyword = "sza"

    # results = count_reddit_posts(json_object)


    # total = 0
    # print("\nğŸ“Š Summary:")
    # for sub, posts in results.items():
    #     print(f"r/{sub}: {len(posts)} posts")
    #     total += len(posts)
    # print(f"\nğŸ¯ Total posts with '{keyword}': {total}")

    # print()
    # print('//////////Filter post date////////////')

    # filtered_results = {}
    # filtered_count = 0
    # for sub, posts in results.items():
    #     filtered_results[sub] = filter_by_date(posts, "2025-03-01", "2025-04-01")
    #     print(f"r/{sub}: {len(filtered_results[sub])} posts between Mar 2025 and Apr 2025")
    #     filtered_count += len(filtered_results[sub])
    
    # print(f"ğŸ“Š Found {filtered_count} posts with '{keyword}' between Mar 2025 and Apr 2025")




if __name__ == "__main__":
    main()






'''
1. Choose the option 
2. Get all the data from kaggle and store it into json object
    - Manipulate data and insert into Kaggle database (25 song each)
3. Get all the reddit posts related to the option (~100 posts/request - repeat to get whole month's data)
    - Count all the mentions 
    - Insert the mention count into the json object 
4. Insert into Reddit database 25 each time (individual counts of ~25 songs)

Reference:
Aggregating subreddits: https://www.reddit.com/r/redditdev/comments/uwfrc7/the_rate_limit_of_60_requests_per_minute_might/
Increased Rate Limits: https://www.reddit.com/r/redditdev/comments/14nbw6g/updated_rate_limits_going_into_effect_over_the/
    Free API access rates are as follows:
        - 100 queries per minute per OAuth client id if you are using OAuth authentication
        - 10 queries per minute if you are not using OAuth authentication
PRAW Rate limit Headers: https://www.reddit.com/r/redditdev/comments/7muatr/praw_rate_limit_headers/


4/18
- Successfully filtered data from Kaggle (desired column and val in column)
    - 50 per country
TODO: Make an option "save only" to run at least four times 
    - Retrieve Kaggle data (maybe 2 countries?) and save it as json object
    - Retrieve Reddit data by reading the json object 
    - Store the data into database
'''